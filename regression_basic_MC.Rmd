---
title: "Matemática da regressão linear - Básico"
author: "Luiz Paulo Tavares"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Dependências computacionais 

```{r}
rm(list = ls())
graphics.off()
set.seed(123)

pacman::p_load(tidyverse,
               stats)
```

## **Da equação da reta à regressão linear** 

É intuitivo iniciar a exposição de regressão linear no $\mathbb{R}^2$ tomando como ponto de partida a equação da reta: 

$$
\mathbb{R}^2 = \mathbb{R} \times \mathbb{R} = \{(x,y) \mid x,y \in \mathbb{R}\}
$$

$$
y = a + bx
$$

Estatisticamente, uma dada regressão linear simples busca estimar a inclinação da reta, ou seja, estimar uma reta que minimiza os pontos entre $x$ e $y$ de intersecção dado uma determinada dispersão. Assim, pode-se especificar um modelo de regressão tomando uma variável dependente $Y\in \mathbb{R}^2$ dado um vetor $x = (x_{1},...,x_{d}) \in \mathbb{R}^d$: 

$$
\mathbb{R}^d = \mathbb{R} \times \mathbb{R} \times \dots \times \mathbb{R} = \{(x_1, x_2, \dots, x_d) \mid x_1, x_2, \dots, x_d \in \mathbb{R}\}
$$
Em uma notação convencional (isto é, não matricial) e simplificada, temos: 

$$
Y_{i} = \beta_{1}+\beta_{2}X_{i}+\mu_{i}
$$

Observe a adição de $\mu$ representando os resíduos da regressão. Os quais são em essência a diferenças entre os valores observados e estimados. Por sua vez, ${\beta_{1}}$ e ${\beta_{2}}$ representam o intercepto e coeficiente angular da equação da reta, respectivamente. Assim, estima-se dado uma amostra $n$ com variabilidade: 

$$
Y_{i} = \hat{\beta_{1}}+{\hat{\beta_{2}}X_{i}}+{\hat{\mu}_{i}}
$$
Os resíduos: 

$$
{\hat{\mu}_{i}} = {Y} - {\hat{Y_{i}}}
$$
Portanto: 

$$
\hat{\mu_{i}} = Y_{i} - {\hat{\beta}_{1}}-{\hat{\beta}_{2}X_{i}}
$$

Do exposto, desenvolve-se por consequência a busca por encontrar ou, melhor, minimizar os resíduos encontrando valores próximos de $Y$
 observado. Critério bem difundido na literatura, pelo menos desde Friedrich Gauss, é o método dos mínimos quadrados. O qual, como suguere, busca minimizar os resíduos (GUJARATI & PORTER, 2011). Tomando como ponto de partida:

$$
{\sum_{i =1}^n \mu^2_{i}} = \sum(Y_i - {\hat{Y_{i}}})^2
$$
$$
= \sum_{i=1}^n(Y_i - {\hat{\beta_{1}}}-{\hat{\beta}_{2}}X_{i})^2
$$

## **Dispersão**




Posteriormente

Pode-se estimar considerando apenas uma variável indepedente como segue tomando $X$ ~ $N(0,\sigma^2)$: 

```{r}
# Modelo de regresssão linear simples 
y = rnorm(n = 1000)
x = rnorm(n = 1000, mean = 0, sd = 1) 

test = y - x
sum(test)

bd = data.frame(depedente = y, explicativa = x)

ggplot2::ggplot(bd)+
  aes(y, x)+
  geom_point(pch = 19)+
  geom_smooth(method = "lm")

model_lm <- stats::lm(formula = y ~ x ) %>% print()
```

Assim, estima-se $\hat{\beta_{0}}$ e $\hat{\beta_{1}}$, ou seja, o intercepto e o coeficiente angular da reta de regressão linear, respectivamente. 

A inclinação da reta pode dada como segue: 

$$
\hat{\beta_{2}} = \frac{rs_{y}}{s_{x}}
$$
Ou seja, dado a correlação de Pearson $r$ entre $x$ e $y$ multiplicado pelo desvio padrão $s_{y}$  dividido pelo desvio padrão de $s_{x}$. 


E o intercepto 

```{r}
b_2 = (stats::cor(x,y) * stats::sd(y)) / (stats::sd(x))
print(b_2)
```

para b_zero

$$
\beta_{1} = \overline{y} - \beta_{2}\overline{x}
$$
```{r}
b_1 = base::mean(y) - (model_lm[["coefficients"]][["x"]] *base::mean(x)) 
print(b_1)
```

Ou seja, uma alta dispersão, principalmente, seguindo um padrão não linear pode em muito prejudicar o ajuste de uma reta linear entre $x$ e $y$ de minimização. Indo além, pode-se tomar a dispersão como $\mu$: 


$$
y(x):= E[Y|X = x]
$$
